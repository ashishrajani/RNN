{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "\n",
    "from os import listdir\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Image of Yaktocat](https://i.postimg.cc/QNp5pFwd/sdf.png)\n",
    "\n",
    "The layers are as follows:\n",
    "- **Tokenize:** This is not a layer for LSTM network but a mandatory step of converting our words into tokens (integers)\n",
    "- **Embedding Layer:** that converts our word tokens (integers) into embedding of specific size\n",
    "- **LSTM Layer:** defined by hidden state dims and number of layers\n",
    "- **Fully Connected Layer:** that maps output of LSTM layer to a desired output size\n",
    "- **Sigmoid Activation Layer:** that turns all output values in a value between 0 and 1\n",
    "- **Output:** Sigmoid output from the last timestep is considered as the final output of this network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = 64\n",
    "        self.output_dim = 2\n",
    "\n",
    "        self.hidden_dim = 25\n",
    "        self.no_layers = 2\n",
    "\n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim,\n",
    "                           num_layers=self.no_layers, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "\n",
    "        embeds = x\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        # reshape to be batch_size first\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim))\n",
    "        c0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim))\n",
    "        hidden = (h0, c0)\n",
    "        return hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preparing dataset object and embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class RnnDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, model = None):\n",
    "        self.data = data\n",
    "        self.length = len(self.data)\n",
    "        self.words2index = {}\n",
    "\n",
    "        padding_string = \"PADDING___PADDING\"\n",
    "        threshold = 5\n",
    "        sentences = []\n",
    "\n",
    "        #max size sentence size for each label (total mean of the size of all the sentences of that label)\n",
    "        max_size = 0\n",
    "        cnt = Counter()\n",
    "\n",
    "        for l,s in data:\n",
    "            max_size += len(s)\n",
    "            for w in s:\n",
    "                cnt[w] += 1\n",
    "\n",
    "        max_size = int(max_size / self.length)\n",
    "\n",
    "        #Add padding\n",
    "        new_train_data = []\n",
    "        for l,s in data:\n",
    "            new_s = [w if cnt[w] > threshold else \"unk\" for w in s]\n",
    "            if len(new_s) < max_size:\n",
    "                pad = [padding_string] * (max_size - len(new_s))\n",
    "                new_s = new_s + pad\n",
    "            elif len(new_s) > max_size:\n",
    "                new_s = new_s[0:max_size]\n",
    "            new_train_data.append((l,new_s))\n",
    "            sentences.append(new_s)\n",
    "\n",
    "        self.data = new_train_data\n",
    "\n",
    "        if model is None :\n",
    "            self.model = Word2Vec(sentences=sentences, min_count=-1, workers=4, size=64)\n",
    "            self.model.build_vocab(cnt.keys(), update=True)\n",
    "            self.model.save(\"w2vemb_rnn.model\")\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label,sentence = self.data[index]\n",
    "        sentence = [w if w in self.model else \"unk\" for w in sentence]\n",
    "        out = torch.tensor(self.model.wv[sentence],dtype=torch.float)\n",
    "        label = torch.tensor(label,dtype=torch.long)\n",
    "        return out, label\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "preprocessing sentences: removing symbols, extra spaces, digits and tokenizing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def preprocess_token(s):\n",
    "    # Removing all non-word character except letters and numbers\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "\n",
    "    # Replacing all extra whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "\n",
    "    # replacing digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def tokenize(x_train):\n",
    "    word_list = []\n",
    "\n",
    "    for word in x_train.lower().split():\n",
    "        word = preprocess_token(word)\n",
    "        if word != '':\n",
    "            word_list.append(word)\n",
    "\n",
    "    return word_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preparing dataset instance by reading reviews and labels from files, followed by performing above defined preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_rnn_dataset(path : str, optional_file : str = None):\n",
    "    num_folder = 2\n",
    "    i = 0\n",
    "    data = []\n",
    "    cwd = os.getcwd()\n",
    "    while i < num_folder:\n",
    "        label = i\n",
    "        folder_name = cwd + \"/\" + path + \"/\" + str(i)\n",
    "        file_names = listdir(folder_name)\n",
    "        for file_name in file_names:\n",
    "            file_path = folder_name + \"/\" + file_name\n",
    "            f = open(file_path,\"r\",encoding=\"utf-8\")\n",
    "            s = tokenize(f.read())\n",
    "            data.append((label,s))\n",
    "            f.close()\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    if optional_file is None :\n",
    "        return RnnDataset(data)\n",
    "\n",
    "    return RnnDataset(data, Word2Vec.load(optional_file))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train_rnn(rnn_instance: RNN, dataloader, epochs = 10):\n",
    "    # loss and optimization functions\n",
    "    lr=0.005\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(rnn_instance.parameters(), lr=lr)\n",
    "\n",
    "    clip = 5\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        iterator = tqdm(dataloader)\n",
    "\n",
    "        for inputs, labels in iterator:\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            # h = tuple([each.data for each in h])\n",
    "            output = rnn_instance(inputs)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, labels)\n",
    "            rnn_instance.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(rnn_instance.parameters(), clip)\n",
    "            optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# initializing RNN instance\n",
    "rnn_inst = RNN()\n",
    "\n",
    "# reading data and preparing dataset instance\n",
    "rnn_dataset = get_rnn_dataset(\"/train\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# preparing training, validation and test set from the above obtained dataset\n",
    "# train= 80% | valid = 10% | test = 10%\n",
    "\n",
    "train_size = int(0.8 * len(rnn_dataset))\n",
    "rem_size = len(rnn_dataset) - train_size\n",
    "train_dataset, rem_dataset = torch.utils.data.random_split(rnn_dataset, [train_size, rem_size])\n",
    "\n",
    "valid_size = int(rem_size/2)\n",
    "test_size = rem_size - valid_size\n",
    "validation_dataset, test_dataset = torch.utils.data.random_split(rem_dataset, [valid_size, test_size])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# preparing dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "validation_loader = DataLoader(validation_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d10be486d0048b598ea83fae16cf088"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env-TM2020/lib/python3.7/site-packages/ipykernel_launcher.py:49: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4737755ac6d4c519386e67ab0df8d07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edc58e68e9c14d21ae2aacc356af6b4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b43547ec0886459496e3e911d6fd13a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecd76e79ca674c589a27f551c1e7efd6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3de0ab6f24d34812a30b74dc0c5fe708"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08e6630be0654e0cb6affe0f19b1d962"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc10ee59f64b40f7afbd476eaaff694e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af6ac3901c02472488369a7698ce469b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8243b8e25b3441e9870fe6ad1396de7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_rnn(rnn_inst, train_loader, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# save model after training\n",
    "torch.save(rnn_inst.state_dict(), \"rnn.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "RNN(\n  (lstm): LSTM(64, 25, num_layers=2, batch_first=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=25, out_features=2, bias=True)\n  (sig): Sigmoid()\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing new RNN instance using pretrained model\n",
    "rnn_loaded = RNN()\n",
    "rnn_loaded.load_state_dict(torch.load(\"rnn.pt\"))\n",
    "rnn_loaded.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# evaluation function to evaluate the model's performance w.r.t. dataset\n",
    "def evaluate(clf, data_loader):\n",
    "\n",
    "    true_labels = []\n",
    "    inf_labels = []\n",
    "\n",
    "    for data, labels in data_loader:\n",
    "        out = clf(data)\n",
    "        cls = torch.argmax(F.softmax(out, dim=1), dim=1)\n",
    "        inf_labels.extend(cls.detach().numpy().tolist())\n",
    "        true_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "    return accuracy_score(true_labels, inf_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env-TM2020/lib/python3.7/site-packages/ipykernel_launcher.py:49: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8073333333333333"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(rnn_loaded, validation_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env-TM2020/lib/python3.7/site-packages/ipykernel_launcher.py:49: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8093333333333333"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(rnn_loaded, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-env-TM2020-py",
   "language": "python",
   "display_name": "Python [conda env:env-TM2020] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}